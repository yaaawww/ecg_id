{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a5fab2e-75e4-45f0-a158-6c0484664513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Mobile Net\n",
    "from MobileNetV2 import mobilenet_v2\n",
    "# dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "# dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "# Util\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61e43e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "input_size = 224\n",
    "batch_size = 64\n",
    "n_worker = 0\n",
    "lr = 0.001\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "929e2380-74d0-42e8-bafe-9f3c1d599540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集函数\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "def train_test_split(\n",
    "    dataset: Dataset,\n",
    "    test_ratio: float,\n",
    "    seed: Optional[int] = None,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"Splits a dataset into random train and test subsets.\n",
    "    Args:\n",
    "        dataset (Dataset): dataset.\n",
    "        test_ratio (float): test proportion (between 0 and 1).\n",
    "        seed (int, optional): seed. Defaults to None.\n",
    "    Returns:\n",
    "        Tuple[Dataset, Dataset]: train and test datasets.\n",
    "    \"\"\"\n",
    "    # Define generator\n",
    "    generator = torch.Generator()\n",
    "    if seed is not None:\n",
    "        generator.manual_seed(seed)\n",
    "\n",
    "    # Define lengths of subsets\n",
    "    train_ratio = 1 - test_ratio\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    lengths = [train_size, test_size]\n",
    "\n",
    "    # Split\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths, generator)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "916c1c6f-6228-4d64-928d-68c045094bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成训练数据集\n",
    "set_path = \"image/train_image\"\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size, scale=(0.2, 1.0)), \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "])\n",
    "dataset = datasets.ImageFolder(set_path, transform=data_transform)\n",
    "train_dataset, val_dataset = train_test_split(dataset, 0.2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63a6157c-189f-46fc-9698-f8b99af5efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=n_worker, pin_memory=True)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, \n",
    "    num_workers=n_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51152818-ed57-45d7-b1cb-355112970b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"image/test_image\"\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size, scale=(0.2, 1.0)), \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "])\n",
    "test_dataset = datasets.ImageFolder(test_path, transform=data_transform)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, \n",
    "    num_workers=n_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7b0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mobilenet_v2()\n",
    "save_path = 'save_model/ecg_id.pt'\n",
    "model = torch.load(save_path)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e7a13a-3f06-4de6-9e6a-cea34fa8c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        start_train = time.time()\n",
    "        # clear the grad\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # loss function\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af63268-0bcc-4404-a89d-4c17d6f1b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):       \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    gt_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            gt_labels.append(label.cpu().data.numpy())\n",
    "            pred_labels.append(preds.cpu().data.numpy())\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "    val_loss = val_loss/len(val_loader.dataset)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n",
    "    acc = np.sum(gt_labels==pred_labels)/len(pred_labels)\n",
    "    writer.add_scalar(\"Acc/train\", acc, epoch)\n",
    "    print('Epoch: {} \\tValidation Loss: {:.6f}, Accuracy: {:6f}'.format(epoch, val_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24a8cfb1-cfaa-42ac-9fdd-fd1d708ac2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):       \n",
    "    model.eval()\n",
    "    gt_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            gt_labels.append(label.cpu().data.numpy())\n",
    "            pred_labels.append(preds.cpu().data.numpy())\n",
    "    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n",
    "    acc = np.sum(gt_labels==pred_labels)/len(pred_labels)\n",
    "    print('Epoch: {} Accuracy: {:6f}'.format(epoch, acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3e2e2a-0723-4ceb-97ae-ec1f28316dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.111609\n",
      "Epoch: 0 Accuracy: 0.547222\n",
      "Epoch: 1 \tTraining Loss: 0.094907\n",
      "Epoch: 1 Accuracy: 0.560556\n",
      "Epoch: 2 \tTraining Loss: 0.089110\n",
      "Epoch: 2 Accuracy: 0.521944\n",
      "Epoch: 3 \tTraining Loss: 0.122679\n",
      "Epoch: 3 Accuracy: 0.463611\n",
      "Epoch: 4 \tTraining Loss: 0.143145\n",
      "Epoch: 4 Accuracy: 0.494444\n",
      "Epoch: 5 \tTraining Loss: 0.161332\n",
      "Epoch: 5 Accuracy: 0.564722\n",
      "Epoch: 6 \tTraining Loss: 0.081993\n",
      "Epoch: 6 Accuracy: 0.570278\n",
      "Epoch: 7 \tTraining Loss: 0.070712\n",
      "Epoch: 7 Accuracy: 0.578611\n",
      "Epoch: 8 \tTraining Loss: 0.105623\n",
      "Epoch: 8 Accuracy: 0.575278\n",
      "Epoch: 9 \tTraining Loss: 0.093387\n",
      "Epoch: 9 Accuracy: 0.583889\n",
      "Epoch: 10 \tTraining Loss: 0.102049\n",
      "Epoch: 10 Accuracy: 0.596111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test(epoch) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.9\u001b[39m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 14\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     15\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     16\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss, epoch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    if test(epoch) > 0.9:\n",
    "        break\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
